# ğŸ¤– RAG Sistemi DetaylÄ± AÃ§Ä±klama

## ğŸ“š RAG Nedir?

**RAG (Retrieval-Augmented Generation)** = **Bilgi Getirme + Ãœretken AI**

Basit anlatÄ±mla: LLM'e soru sormadan Ã¶nce, **ilgili belgeleri bulup** LLM'e context olarak veriyoruz. BÃ¶ylece LLM sadece kendi bilgisine deÄŸil, **verdiÄŸimiz belgelere** dayanarak cevap veriyor.

---

## ğŸ¯ Neden RAG KullanÄ±yoruz?

### âŒ Normal LLM Problemi:
```
KullanÄ±cÄ±: "Hacettepe YZ bÃ¶lÃ¼mÃ¼ ne zaman kuruldu?"
LLM: "Bilmiyorum" veya "2015'te kurulmuÅŸ olabilir" (uydurma!)
```

### âœ… RAG ile Ã‡Ã¶zÃ¼m:
```
1. Sistem: "YZ bÃ¶lÃ¼mÃ¼" ile ilgili belgeleri bul
2. Sistem: Bulunan belgeleri LLM'e ver
3. LLM: "Belgelere gÃ¶re 2019'da kurulmuÅŸ" (doÄŸru!)
```

---

## ğŸ—ï¸ RAG Sistemi Mimarisi

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG PIPELINE                             â”‚
â”‚                                                             â”‚
â”‚  1. INDEXING (Offline - Belgeler yÃ¼klenirken)              â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚     â”‚   PDF    â”‚ â†’  â”‚  Chunks  â”‚ â†’  â”‚  Embeddings  â”‚      â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                              â†“              â”‚
â”‚                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚                                      â”‚  Vector DB   â”‚      â”‚
â”‚                                      â”‚  (LanceDB)   â”‚      â”‚
â”‚                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                             â”‚
â”‚  2. RETRIEVAL (Online - Soru sorulduÄŸunda)                 â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚     â”‚  Soru    â”‚ â†’  â”‚ Embeddingâ”‚ â†’  â”‚Vector Search â”‚      â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                              â†“              â”‚
â”‚                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚                                      â”‚ Top-K Chunks â”‚      â”‚
â”‚                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                             â”‚
â”‚  3. GENERATION (Online - YanÄ±t Ã¼retme)                     â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚     â”‚ Context  â”‚ +  â”‚  Soru    â”‚ â†’  â”‚     LLM      â”‚      â”‚
â”‚     â”‚ (Chunks) â”‚    â”‚          â”‚    â”‚ (llama3.1)   â”‚      â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                              â†“              â”‚
â”‚                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚                                      â”‚    YanÄ±t     â”‚      â”‚
â”‚                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” AdÄ±m AdÄ±m RAG SÃ¼reci

### ğŸ“¥ AÅAMA 1: INDEXING (Belge YÃ¼kleme)

Bu aÅŸama **sadece bir kez**, belgeler sisteme eklendiÄŸinde yapÄ±lÄ±r.

#### 1.1. PDF â†’ Metin
```python
# pipeline/pdf_loader.py
PDF DosyasÄ± â†’ PyMuPDF â†’ Ham Metin
```

#### 1.2. Metin Temizleme
```python
# pipeline/text_cleaner.py
Ham Metin â†’ Unicode normalize â†’ Sayfa numarasÄ± kaldÄ±r â†’ Temiz Metin
```

#### 1.3. Chunking (Semantic)
```python
# pipeline/chunker.py
Temiz Metin â†’ Semantic Chunking â†’ Chunk Listesi

Ã–rnek:
"Hacettepe Ãœniversitesi 1967'de kuruldu. Ankara'da bulunur..."
â†“
Chunk 1: "Hacettepe Ãœniversitesi 1967'de kuruldu. Ankara'da bulunur."
Chunk 2: "TÄ±p FakÃ¼ltesi Ã§ok Ã¼nlÃ¼dÃ¼r. BirÃ§ok doktor yetiÅŸtirmiÅŸtir."
```

#### 1.4. Embedding (VektÃ¶rleÅŸtirme)
```python
# pipeline/embedder.py
Her Chunk â†’ Ollama (bge-m3) â†’ 1024 boyutlu vektÃ¶r

Ã–rnek:
"Hacettepe 1967'de kuruldu" â†’ [0.23, -0.45, 0.67, ..., 0.12] (1024 sayÄ±)
```

**Embedding nedir?**
- Metni sayÄ±lara Ã§evirme
- Anlamsal olarak benzer metinler, benzer vektÃ¶rlere sahip olur
- Ã–rnek:
  - "Hacettepe kuruldu" â†’ [0.2, 0.5, ...]
  - "Hacettepe aÃ§Ä±ldÄ±" â†’ [0.21, 0.49, ...] (Ã§ok benzer!)
  - "Elma yedim" â†’ [0.9, -0.3, ...] (Ã§ok farklÄ±!)

#### 1.5. Vector Store (VeritabanÄ±na Kaydet)
```python
# pipeline/vector_store.py
LanceDB'ye kaydet:
{
  "id": "uuid-123",
  "text": "Hacettepe 1967'de kuruldu",
  "embedding": [0.23, -0.45, ...],
  "source": "tarih.pdf"
}
```

---

### ğŸ” AÅAMA 2: RETRIEVAL (Bilgi Getirme)

KullanÄ±cÄ± soru sorduÄŸunda bu aÅŸama Ã§alÄ±ÅŸÄ±r.

#### 2.1. Soru Embedding'i
```python
# pipeline/rag_engine.py â†’ retrieve_context()

KullanÄ±cÄ± Sorusu: "Hacettepe ne zaman kuruldu?"
â†“
Ollama (bge-m3) â†’ [0.19, 0.52, ..., 0.08] (1024 boyutlu vektÃ¶r)
```

#### 2.2. Vector Search (Benzerlik AramasÄ±)
```python
# pipeline/vector_store.py â†’ search_vectors()

Soru VektÃ¶rÃ¼: [0.19, 0.52, ...]
â†“
LanceDB'de ara (Cosine Similarity veya L2 Distance)
â†“
En benzer TOP_K chunk'Ä± bul (varsayÄ±lan: 5)

SonuÃ§:
1. "Hacettepe 1967'de kuruldu" (distance: 0.05) âœ… Ã‡ok benzer
2. "Ankara'da bulunur" (distance: 0.12) âœ… Benzer
3. "TÄ±p FakÃ¼ltesi Ã¼nlÃ¼" (distance: 0.25) âœ… Az benzer
4. "KÃ¼tÃ¼phane bÃ¼yÃ¼k" (distance: 0.40) âš ï¸ Uzak
5. "Kafeterya var" (distance: 0.55) âš ï¸ Ã‡ok uzak
```

**Benzerlik Metrikleri:**
- **Cosine Distance**: 0 = aynÄ±, 1 = tamamen farklÄ±
- **L2 (Euclidean) Distance**: KÃ¼Ã§Ã¼k = benzer, bÃ¼yÃ¼k = farklÄ±

#### 2.3. Context OluÅŸturma
```python
# pipeline/rag_engine.py â†’ retrieve_context()

Top-5 Chunk'larÄ± birleÅŸtir:
context = """
--- Chunk from tarih.pdf ---
Hacettepe Ãœniversitesi 1967 yÄ±lÄ±nda kurulmuÅŸtur.

--- Chunk from konum.pdf ---
Ankara'da SÄ±hhiye'de bulunur.

--- Chunk from fakulteler.pdf ---
TÄ±p FakÃ¼ltesi Ã§ok Ã¼nlÃ¼dÃ¼r.
"""
```

---

### ğŸ¨ AÅAMA 3: GENERATION (YanÄ±t Ãœretme)

#### 3.1. Prompt OluÅŸturma
```python
# pipeline/rag_engine.py â†’ generate_answer()

prompt = f"""
{SYSTEM_PROMPT}  # Sistem talimatlarÄ±

**CONTEXT (BAÄLAM):**
{context}  # Bulunan chunk'lar

**SORU:**
{query}  # KullanÄ±cÄ± sorusu

**YANIT:**
"""
```

**Tam Prompt Ã–rneÄŸi:**
```
Sen Hacettepe_Akademik_Asistan'sÄ±n.
Sadece verilen CONTEXT'i kullan. Uydurma yapma.

**CONTEXT (BAÄLAM):**
--- Chunk from tarih.pdf ---
Hacettepe Ãœniversitesi 1967 yÄ±lÄ±nda kurulmuÅŸtur.

--- Chunk from konum.pdf ---
Ankara'da SÄ±hhiye'de bulunur.

**SORU:**
Hacettepe ne zaman kuruldu?

**YANIT:**
```

#### 3.2. LLM Ã‡aÄŸrÄ±sÄ±
```python
# Ollama API'ye gÃ¶nder
response = requests.post(
    "http://127.0.0.1:11434/api/generate",
    json={
        "model": "llama3.1:8b",
        "prompt": prompt,
        "temperature": 0.1  # DÃ¼ÅŸÃ¼k = daha deterministik
    }
)
```

**Temperature nedir?**
- `0.0` = Robotik, her zaman aynÄ± cevap
- `0.1` = Ã‡ok tutarlÄ±, fakta dayalÄ± (RAG iÃ§in ideal)
- `0.7` = YaratÄ±cÄ±
- `1.0+` = Ã‡ok yaratÄ±cÄ±, bazen saÃ§ma

#### 3.3. YanÄ±t Formatla
```python
final_answer = """
Hacettepe Ãœniversitesi 1967 yÄ±lÄ±nda kurulmuÅŸtur.

**Kaynaklar:**
tarih.pdf, konum.pdf

**KullanÄ±lan Chunklar:**
[1] (tarih.pdf):
Hacettepe Ãœniversitesi 1967 yÄ±lÄ±nda kurulmuÅŸtur. Ankara'da...

[2] (konum.pdf):
Ankara'da SÄ±hhiye'de bulunur. KampÃ¼s Ã§ok bÃ¼yÃ¼ktÃ¼r...
"""
```

---

## ğŸ§  RAG vs Normal LLM

### Normal LLM (GPT, Claude, vb.)
```
KullanÄ±cÄ±: "Hacettepe YZ bÃ¶lÃ¼mÃ¼ ne zaman kuruldu?"
â†“
LLM (kendi bilgisi): "Bilmiyorum" veya uydurma yapar
```

**Sorunlar:**
- âŒ GÃ¼ncel bilgi yok (eÄŸitim verisi eski)
- âŒ Ã–zel bilgi yok (ÅŸirket iÃ§i belgeler)
- âŒ Hallucination (uydurma) riski yÃ¼ksek

### RAG Sistemi
```
KullanÄ±cÄ±: "Hacettepe YZ bÃ¶lÃ¼mÃ¼ ne zaman kuruldu?"
â†“
1. Belgelerde ara â†’ "YZ bÃ¶lÃ¼mÃ¼ 2019'da kuruldu" bulundu
2. LLM'e ver â†’ "Belgeye gÃ¶re 2019'da kuruldu" (doÄŸru!)
```

**Avantajlar:**
- âœ… GÃ¼ncel bilgi (belgeler gÃ¼ncellenebilir)
- âœ… Ã–zel bilgi (kendi belgeleriniz)
- âœ… Kaynak gÃ¶sterimi (hangi belgeden geldi)
- âœ… Hallucination azalÄ±r (belgeye dayalÄ±)

---

## âš™ï¸ Sistem KonfigÃ¼rasyonu

### `config.py` - RAG Parametreleri

```python
# Embedding Modeli
EMBEDDING_MODEL = "bge-m3:latest"  # 1024 boyutlu vektÃ¶r

# LLM Modeli
LLM_MODEL = "llama3.1:8b"  # TÃ¼rkÃ§e destekli

# RAG Parametreleri
TOP_K = 5                    # KaÃ§ chunk getirilecek
MIN_SCORE_THRESHOLD = 0.35   # Minimum benzerlik skoru

# LLM Parametreleri
temperature = 0.1            # DÃ¼ÅŸÃ¼k = daha tutarlÄ±
```

### TOP_K AyarÄ±

**TOP_K = 3** (Az chunk)
- âœ… HÄ±zlÄ± yanÄ±t
- âœ… OdaklÄ± cevap
- âŒ BazÄ± bilgiler kaÃ§abilir

**TOP_K = 5** (Orta) - **Ã–NERÄ°LEN**
- âœ… Dengeli
- âœ… Yeterli context
- âœ… Makul hÄ±z

**TOP_K = 10** (Ã‡ok chunk)
- âœ… KapsamlÄ± bilgi
- âŒ YavaÅŸ
- âŒ LLM kafasÄ± karÄ±ÅŸabilir (Ã§ok fazla bilgi)

---

## ğŸ¯ Ã–rnek Senaryo: Tam AkÄ±ÅŸ

### Senaryo: "Hacettepe'de kaÃ§ fakÃ¼lte var?"

#### 1. Indexing (Ã–nceden yapÄ±lmÄ±ÅŸ)
```
belgeler/fakulteler.pdf yÃ¼klendi
â†“
Chunk 1: "Hacettepe'de 15 fakÃ¼lte vardÄ±r. TÄ±p, MÃ¼hendislik..."
Chunk 2: "TÄ±p FakÃ¼ltesi 1967'de kuruldu..."
Chunk 3: "MÃ¼hendislik FakÃ¼ltesi 1970'te aÃ§Ä±ldÄ±..."
â†“
Her chunk â†’ Embedding â†’ LanceDB'ye kayÄ±t
```

#### 2. Retrieval (Soru sorulunca)
```
Soru: "Hacettepe'de kaÃ§ fakÃ¼lte var?"
â†“
Soru â†’ Embedding â†’ [0.34, 0.67, ...]
â†“
LanceDB'de ara
â†“
Top-5 Chunk:
1. "Hacettepe'de 15 fakÃ¼lte vardÄ±r..." (distance: 0.03) âœ…
2. "FakÃ¼lteler ÅŸunlardÄ±r: TÄ±p, MÃ¼h..." (distance: 0.08) âœ…
3. "TÄ±p FakÃ¼ltesi 1967'de kuruldu..." (distance: 0.15) âœ…
4. "KampÃ¼s Ã§ok bÃ¼yÃ¼ktÃ¼r..." (distance: 0.35) âš ï¸
5. "KÃ¼tÃ¼phane 24 saat aÃ§Ä±k..." (distance: 0.42) âš ï¸
```

#### 3. Generation (YanÄ±t Ã¼retme)
```
Prompt:
---
Sen Hacettepe AsistanÄ±'sÄ±n. Sadece CONTEXT'i kullan.

CONTEXT:
- Hacettepe'de 15 fakÃ¼lte vardÄ±r. TÄ±p, MÃ¼hendislik...
- FakÃ¼lteler ÅŸunlardÄ±r: TÄ±p, MÃ¼h...
- TÄ±p FakÃ¼ltesi 1967'de kuruldu...

SORU: Hacettepe'de kaÃ§ fakÃ¼lte var?

YANIT:
---
â†“
LLM (llama3.1:8b):
"Hacettepe Ãœniversitesi'nde 15 fakÃ¼lte bulunmaktadÄ±r."

**Kaynaklar:** fakulteler.pdf
```

---

## ğŸ”§ Optimizasyon Ä°puÃ§larÄ±

### 1. Chunk Boyutu
```python
CHUNK_SIZE = 4000  # Optimal

# Ã‡ok kÃ¼Ã§Ã¼k (1000): Ã‡ok fazla chunk, context kaybolur
# Ã‡ok bÃ¼yÃ¼k (8000): Az chunk, alakasÄ±z bilgi artar
```

### 2. Overlap
```python
CHUNK_OVERLAP = 200  # Optimal

# Overlap neden Ã¶nemli?
Chunk 1: "...Hacettepe 1967'de kuruldu. Ankara'da bulunur."
Chunk 2: "Ankara'da bulunur. TÄ±p FakÃ¼ltesi Ã¼nlÃ¼dÃ¼r..."
         â†‘ Bu kÄ±sÄ±m overlap (context korunur)
```

### 3. Embedding Modeli
```python
# KÃ¼Ã§Ã¼k model (hÄ±zlÄ± ama az doÄŸru)
EMBEDDING_MODEL = "all-minilm:latest"  # 384 boyut

# Orta model (dengeli) - Ã–NERÄ°LEN
EMBEDDING_MODEL = "bge-m3:latest"  # 1024 boyut

# BÃ¼yÃ¼k model (yavaÅŸ ama Ã§ok doÄŸru)
EMBEDDING_MODEL = "bge-large:latest"  # 1536 boyut
```

---

## ğŸ“Š Performans Metrikleri

### YanÄ±t SÃ¼resi Analizi
```
1. Embedding oluÅŸturma: ~100ms
2. Vector search: ~50ms
3. LLM yanÄ±t Ã¼retme: ~3-5 saniye
---
Toplam: ~3-5 saniye
```

### DoÄŸruluk ArtÄ±ÅŸÄ±
```
Normal LLM: %40-50 doÄŸruluk (uydurma riski)
RAG Sistemi: %85-95 doÄŸruluk (belgeye dayalÄ±)
```

---

## ğŸ› YaygÄ±n Sorunlar ve Ã‡Ã¶zÃ¼mler

### 1. "Bilmiyorum" YanÄ±tÄ±
**Neden:** VeritabanÄ±nda ilgili chunk yok
**Ã‡Ã¶zÃ¼m:** 
- Daha fazla belge ekle
- TOP_K artÄ±r (5 â†’ 7)
- Chunk boyutunu ayarla

### 2. YanlÄ±ÅŸ YanÄ±t
**Neden:** AlakasÄ±z chunk'lar getirildi
**Ã‡Ã¶zÃ¼m:**
- Embedding modelini iyileÅŸtir
- MIN_SCORE_THRESHOLD ekle
- Chunk kalitesini artÄ±r (semantic chunking)

### 3. YavaÅŸ YanÄ±t
**Neden:** LLM Ã§ok bÃ¼yÃ¼k veya TOP_K Ã§ok yÃ¼ksek
**Ã‡Ã¶zÃ¼m:**
- TOP_K azalt (5 â†’ 3)
- Daha kÃ¼Ã§Ã¼k LLM kullan
- GPU kullan

---

## ğŸ“ SonuÃ§

RAG sistemi 3 basit adÄ±mdan oluÅŸur:

1. **ğŸ“¥ Index** - Belgeleri vektÃ¶rlere Ã§evir, sakla
2. **ğŸ” Retrieve** - Soruya benzer chunk'larÄ± bul
3. **ğŸ¨ Generate** - Chunk'larÄ± LLM'e ver, yanÄ±t al

**AvantajlarÄ±:**
- âœ… GÃ¼ncel bilgi
- âœ… Ã–zel bilgi (kendi belgeleriniz)
- âœ… Kaynak gÃ¶sterimi
- âœ… Hallucination azalÄ±r
- âœ… Offline Ã§alÄ±ÅŸabilir (Ollama sayesinde)

**Hacettepe RAG Sistemi:**
- Akademik belgeleri iÅŸler
- TÃ¼rkÃ§e destekli
- Semantic chunking ile kaliteli
- Tamamen yerel (gizlilik)

---

**Daha fazla bilgi iÃ§in:**
- `pipeline/rag_engine.py` - RAG implementasyonu
- `README.md` - Genel dokÃ¼mantasyon
- `SEMANTIC_CHUNKING.md` - Chunking detaylarÄ±
